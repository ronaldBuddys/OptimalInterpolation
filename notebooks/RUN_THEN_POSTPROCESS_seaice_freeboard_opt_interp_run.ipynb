{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OloXur9Tew-W",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GF0GW-sOk-2O",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34DDY7AYW-BL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gpflow==2.5.2\n",
    "!pip install pyproj\n",
    "# NOTE: installing cartopy via pip just to avoid import error\n",
    "# - cartopy should be installed via: conda install -c conda-forge cartopy\n",
    "# !pip install cartopy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HsodxyWW9Qx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "config / parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5afwNR9W8q1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "branch_name = \"sparse_dev\"\n",
    "# directory on google drive where to \n",
    "# work_sub_dir = [\"MyDrive\", \"workspace\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJLFW9rgWK3x",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "mount google drive (use to save results)  - requires login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJsq2he6RqVK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "from google.colab import drive\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "gdrive_mount = '/content/gdrive'\n",
    "# # requires giving access to google drive account\n",
    "drive.mount(gdrive_mount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owtMH_NNWSqf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "git pull repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhW6ZpIpWN7o",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# change 'workspace' as needed\n",
    "# work_dir = os.path.join(gdrive_mount, *[\"MyDrive\", \"workspace\"])\n",
    "work_dir = \"/content\"\n",
    "\n",
    "# change to working directory\n",
    "# os.chdir(work_dir)\n",
    "assert os.path.exists(work_dir), f\"workspace directory: {work_dir} does not exist\"\n",
    "os.chdir(work_dir)\n",
    "\n",
    "# !git clone https://github.com/William-gregory/OptimalInterpolation.git\n",
    "# url suffix for cloning repp\n",
    "url = \"https://github.com/ronaldBuddys/OptimalInterpolation.git\"\n",
    "\n",
    "# repository directory\n",
    "repo_dir = os.path.join(work_dir, os.path.basename(url))\n",
    "repo_dir = re.sub(\"\\.git$\", \"\", repo_dir)\n",
    "\n",
    "# TODO: put a try except here \n",
    "# clone the repo\n",
    "\n",
    "try:\n",
    "    git_clone = subprocess.check_output( [\"git\", \"clone\", url] , shell=False)\n",
    "except Exception as e:\n",
    "    # get non-zero exit status 128: if the repo already exists?\n",
    "    print(e)\n",
    "\n",
    "print(f\"changing directory to: {repo_dir}\")\n",
    "\n",
    "os.chdir(repo_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8j2KBZ0OZHBM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Change branch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOoGWoAYa83x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# --\n",
    "# change branch - review this\n",
    "# --\n",
    "\n",
    "try:\n",
    "    git_checkout = subprocess.check_output([\"git\", \"checkout\", \"-t\", f\"origin/{branch_name}\"], shell=False)\n",
    "    print(git_checkout.decode(\"utf-8\") )\n",
    "except Exception as e:\n",
    "    git_checkout = subprocess.check_output([\"git\", \"checkout\",  f\"{branch_name}\"], shell=False)\n",
    "    print(git_checkout.decode(\"utf-8\") )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvJbokw2kKng",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# git pull to ensure have the latest\n",
    "git_pull = subprocess.check_output([\"git\", \"pull\"], shell=False)\n",
    "print(git_pull.decode(\"utf-8\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hadf_YaoZXNq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add directory to containing repository to sys.path, so can import as a package\n",
    "# if repo_dir not in sys.path:\n",
    "#     # tmp_dir = os.path.dirname(repo_dir)\n",
    "#     print(f\"adding {repo_dir} to sys.path\")\n",
    "#     sys.path.extend([])\n",
    "\n",
    "if work_dir not in sys.path:\n",
    "    # tmp_dir = os.path.dirname(repo_dir)\n",
    "    print(f\"adding {work_dir} to sys.path\")\n",
    "    sys.path.extend([work_dir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJzPX1NuF8cV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: only downlaod if it does not already exist\n",
    "import gdown\n",
    "import zipfile\n",
    "\n",
    "# print(\"there was some sort of issue downloading the entire folder structure\")\n",
    "print(\"will try downloading the zipped version\")\n",
    "# url = \"https://drive.google.com/file/d/1c7h6HTT-wbCq_ZKBYLJSSln4tanlLEMZ\"\n",
    "# id = \"1c7h6HTT-wbCq_ZKBYLJSSln4tanlLEMZ\"\n",
    "\n",
    "\n",
    "data_dir = os.path.join(repo_dir, \"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# https://drive.google.com/file/d/1djlaZ2EKbm9pNAEt3w58WJtBA4NyQsNE/view?usp=sharing\n",
    "id_zip = [\n",
    "    # {\"id\": \"1ckoowmCwh4tG76sIxXZuVaSSQ0tv8KTU\", \"zip\": \"auxiliary.zip\", \"dirname\": \"aux\"},\n",
    "    {\"id\": \"1djlaZ2EKbm9pNAEt3w58WJtBA4NyQsNE\", \"zip\": \"new_aux.zip\", \"dirname\": \"aux\"},\n",
    "    {\"id\": \"1cIh9lskzmL6C7EYV8lmJJ5YaJgKqOZHT\", \"zip\": \"CS2S3_CPOM.zip\", \"dirname\": \"CS2S3_CPOM\"},\n",
    "    {\"id\": \"1gXsvtxZcWpBALomgeqn9kcfyCtKD3fkz\", \"zip\": \"raw_along_track.zip\", \"dirname\": \"RAW\"},\n",
    "    # legacy data\n",
    "    # {\"id\": \"1HcKZD_F3esIPc2NbWlexvXbVOSAlso9m\", \"zip\": \"aux_legacy.zip\", \"dirname\": \"aux\"},\n",
    "    # {\"id\": \"1Kekh43yTDVJXfSjUrPDV6ZEXXIJhzdXC\", \"zip\": \"CS2S3_CPOM_legacy.zip\", \"dirname\": \"CS2S3_CPOM\"},\n",
    "]\n",
    "\n",
    "# TODO: check if output dir already exists: aux and CS2S3_CPOM\n",
    "for _ in id_zip:\n",
    "    id = _['id']\n",
    "    zip = _['zip']\n",
    "    dirn = _.get('dirname', \"\")\n",
    "\n",
    "    if os.path.exists(os.path.join(data_dir, dirn)):\n",
    "        # print(f\"dir{}\")\n",
    "        continue\n",
    "    # put data in data dir in repository\n",
    "    output = os.path.join(data_dir, zip)\n",
    "    gdown.download(id=id, output=output, use_cookies=False)\n",
    "\n",
    "    # un zip to path\n",
    "    print(\"unzipping\")\n",
    "    with zipfile.ZipFile(output, 'r') as zip_ref:\n",
    "        zip_ref.extractall(path=data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUXf8f_DGFqM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-0IB2pue-_G",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# change to repo_dir so can get git info\n",
    "os.chdir(repo_dir)\n",
    "\n",
    "# set output directory\n",
    "# output_base_dir = os.path.join(gdrive_mount, \"MyDrive\", \"Dissertation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUARB7I3i4EZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# ------------------------------------------------------------\n",
    "# \n",
    "#                   SCRIPT STARTS HERE\n",
    "#\n",
    "# ------------------------------------------------------------\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "# calculate the hyper-parameters for GP on freeboard cover using a config\n",
    "# - date(s)\n",
    "# - window size\n",
    "# - radius of inclusion\n",
    "# - freeboard season\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from OptimalInterpolation import get_data_path, get_path\n",
    "from OptimalInterpolation.sea_ice_freeboard import SeaIceFreeboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5lfUfn2mI8g",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# #  change output_base_dir as needed\n",
    "# output_base_dir = get_path(\"results\")\n",
    "output_base_dir = os.path.join(gdrive_mount, \"MyDrive\", \"Dissertation\")\n",
    "\n",
    "# ---\n",
    "# Parameters\n",
    "# ---\n",
    "\n",
    "grid_res = 50\n",
    "coarse_grid_spacing = 1\n",
    "incl_rad = 300\n",
    "days_ahead = 4\n",
    "days_behind = 4\n",
    "min_inputs = 5\n",
    "hold_out = None\n",
    "bound_length_scales = True\n",
    "prior_mean_method = \"fyi_average\"\n",
    "\n",
    "\n",
    "# post process dict\n",
    "# - let the std used in\n",
    "post_process = {\n",
    "    \"clip_and_smooth\": True,\n",
    "    \"smooth_method\": \"kernel\",\n",
    "    \"std\": coarse_grid_spacing,\n",
    "    \"vmax_map\": {\n",
    "        \"ls_x\": 2 * incl_rad * 1000,\n",
    "        \"ls_y\": 2 * incl_rad * 1000,\n",
    "        \"ls_t\": days_ahead + days_behind + 1,\n",
    "        \"kernel_variance\": 0.1,\n",
    "        \"likelihood_variance\": 0.05\n",
    "    },\n",
    "    \"vmin_map\": {\n",
    "        \"ls_x\": 1,\n",
    "        \"ls_y\": 1,\n",
    "        \"ls_t\": 1e-6,\n",
    "        \"kernel_variance\": 2e-6,\n",
    "        \"likelihood_variance\": 2e-6\n",
    "    }\n",
    "}\n",
    "\n",
    "# ---\n",
    "# input config\n",
    "# ---\n",
    "\n",
    "config = {\n",
    "    \"dates\": [\"20181202\", \"20190102\"], #[\"20190201\", \"20190301\"]\n",
    "    \"output_dir\": os.path.join(output_base_dir, \"run_then_postprocess\"),\n",
    "    \"incl_rad\": incl_rad,\n",
    "    \"days_ahead\": days_ahead,\n",
    "    \"days_behind\": days_behind,\n",
    "    \"data_dir\": \"package\",\n",
    "    \"season\": \"2018-2019\",\n",
    "    \"grid_res\": grid_res,\n",
    "    \"min_inputs\": min_inputs,\n",
    "    \"verbose\": 3,\n",
    "    \"engine\": \"GPflow\",\n",
    "    \"kernel\": \"Matern32\",\n",
    "    \"prior_mean_method\": prior_mean_method,\n",
    "    \"hold_out\": hold_out,\n",
    "    \"load_params\": False,\n",
    "    \"pred_on_hold_out\": False,\n",
    "    \"scale_inputs\": True,\n",
    "    \"scale_outputs\": False,\n",
    "    \"bound_length_scales\": bound_length_scales,\n",
    "    \"append_to_file\": True,\n",
    "    \"overwrite\": False,\n",
    "    \"use_raw_data\": False,\n",
    "    \"store_params\": False,\n",
    "    \"predict_locations\": [\"center_only\"],\n",
    "    \"skip_if_pred_exists\": True,\n",
    "    \"store_loss\": False\n",
    "}\n",
    "\n",
    "# ---\n",
    "# parameters\n",
    "# ---\n",
    "\n",
    "print(\"using config:\")\n",
    "print(json.dumps(config, indent=4))\n",
    "\n",
    "# extract parameters from config\n",
    "\n",
    "season = config.get(\"season\", \"2018-2019\")\n",
    "assert season == \"2018-2019\", \"only can handle data inputs from '2018-2019' season at the moment\"\n",
    "\n",
    "gres = config.get(\"grid_res\", 50)\n",
    "\n",
    "# pop some config parameters\n",
    "data_dir = config.pop(\"data_dir\")\n",
    "verbose = config.pop(\"verbose\", True)\n",
    "dates = config.pop(\"dates\")\n",
    "\n",
    "# -----\n",
    "# initialise SeaIceFreeboard object\n",
    "# -----\n",
    "\n",
    "sifb = SeaIceFreeboard(grid_res=f\"{gres}km\",\n",
    "                       length_scale_name=[\"x\", \"y\", \"t\"],\n",
    "                       verbose=verbose,\n",
    "                       rng_seed=None)\n",
    "\n",
    "# ---\n",
    "# read / load data\n",
    "# ---\n",
    "\n",
    "if data_dir == \"package\":\n",
    "    data_dir = get_data_path()\n",
    "\n",
    "assert os.path.exists(data_dir)\n",
    "\n",
    "sifb.load_data(aux_data_dir=os.path.join(data_dir, \"aux\"),\n",
    "               sat_data_dir=os.path.join(data_dir, \"CS2S3_CPOM\"),\n",
    "               # raw_data_dir=os.path.join(data_dir, \"RAW\"),\n",
    "               season=season)\n",
    "\n",
    "# ---\n",
    "# for each date increment over the model combinations\n",
    "# ---\n",
    "\n",
    "for date in dates:\n",
    "    print(\"|\" * 100)\n",
    "    print(f\"date: {date}\")\n",
    "    print(\"#\" * 10)\n",
    "\n",
    "    # ---\n",
    "    # initial run\n",
    "    # ---\n",
    "\n",
    "    print(\"|*\" * 50)\n",
    "    print(f\"initial run\")\n",
    "\n",
    "    # tmp_dir is where the results will be written to\n",
    "    tmp_dir = sifb.make_temp_dir(incl_rad,\n",
    "                                 days_ahead,\n",
    "                                 days_behind,\n",
    "                                 grid_res,\n",
    "                                 season,\n",
    "                                 coarse_grid_spacing,\n",
    "                                 hold_out,\n",
    "                                 bound_length_scales,\n",
    "                                 prior_mean_method)\n",
    "\n",
    "    # optimise - no post processing\n",
    "    initial_file_suffix = \"\"\n",
    "    sifb.run(date=date,\n",
    "             optimise=True,\n",
    "             coarse_grid_spacing=coarse_grid_spacing,\n",
    "             file_suffix=initial_file_suffix,\n",
    "             post_process=None,\n",
    "             previous_results=None,\n",
    "             tmp_dir=tmp_dir,\n",
    "             **config)\n",
    "\n",
    "    # ---\n",
    "    # apply post processing\n",
    "    # ---\n",
    "\n",
    "    print(\"|*\" * 50)\n",
    "    print(f\"post processing\")\n",
    "    print(f\"date: {date}\")\n",
    "    print(\"#\" * 10)\n",
    "\n",
    "    # for post processing - need results from previous run\n",
    "    previous_results = {\n",
    "        'dir': os.path.join(config['output_dir'], tmp_dir),\n",
    "        'suffix': initial_file_suffix\n",
    "    }\n",
    "\n",
    "    sifb.run(date=date,\n",
    "             optimise=False,\n",
    "             coarse_grid_spacing=1,\n",
    "             file_suffix=\"_postproc\",\n",
    "             post_process=post_process,\n",
    "             previous_results=previous_results,\n",
    "             tmp_dir=tmp_dir,\n",
    "             **config)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}